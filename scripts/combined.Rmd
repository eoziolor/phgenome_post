---
title: "Phgenome"
author: "Elias"
date: "July 16, 2018"
output: html_document
---
Pacific herring genome assembly and annotation
===
# Assembling genome 
* using Supernova 2.0 pipeline and 55x coverage

```{bash}
/pylon5/bi4ifup/eoziolor/program/supernova-2.0.0/supernova-cs/2.0.0/bin/run --id phgenome3 --maxreads 340000000 --fastqs /pylon5/bi4ifup/eoziolor/phgenome/data/raw/ --localcores=28
```

# Generating fasta for the genome

```{bash}
#!/bin/bash

#SBATCH -J fastagen
#SBATCH -o /home/eoziolor/phgenome/scripts/fasta/phg_fasta_%j.o
#SBATCH -e /home/eoziolor/phgenome/scripts/fasta/phg_fasta_%j.o
#SBATCH --time=7-00:00
#SBATCH --mem=60000

my_super=/home/eoziolor/program/supernova-2.0.0/supernova
my_out=/home/eoziolor/phgenome/data/assembly3/outs/assembly
my_fasta=/home/eoziolor/phgenome/data/assembly3/fasta/phgenome

$my_super mkoutput \
--asmdir=$my_out \
--outprefix=$my_fasta \
--style=megabubbles \
--headers=full
```

* I actually don't know if this is the best route for this, so I might generate a pseudohaploid fasta. I don't use the bubbles for anything in the annotation process.

## Generating pseudohaploid fasta file for further use

```{bash}
#!/bin/bash

#SBATCH -J fastagen
#SBATCH -o /home/eoziolor/phgenome/scripts/fasta/phg_fasta_%j.o
#SBATCH -e /home/eoziolor/phgenome/scripts/fasta/phg_fasta_%j.o
#SBATCH --time=0-01:00
#SBATCH --mem=60000

my_super=/home/eoziolor/program/supernova-2.0.0/supernova
my_out=/home/eoziolor/phgenome/data/assembly3/outs/assembly
my_fasta=/home/eoziolor/phgenome/data/assembly3/fasta/phgenome

$my_super mkoutput \
--asmdir=$my_out \
--outprefix=$my_fasta \
--style=pseudohap \
--headers=full
```

## Installing ARCS

```{bash}
git clone https://github.com/bcgsc/arcs.git
./autogen.sh
./configure && make
```

## Installing Longranger
```{bash}
curl -o longranger-2.2.2.tar.gz "http://cf.10xgenomics.com/releases/genome/longranger-2.2.2.tar.gz?Expires=1531828518&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cDovL2NmLjEweGdlbm9taWNzLmNvbS9yZWxlYXNlcy9nZW5vbWUvbG9uZ3Jhbmdlci0yLjIuMi50YXIuZ3oiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE1MzE4Mjg1MTh9fX1dfQ__&Signature=gK9K5cKBIKxPYjcj58Z3BaRx37Hk24JU~c78wzGWOHGNQtHV3T13zeGKalGqvj0xWJoKfZf7YekPV542XrWVwITmwIzt0QLTh4yAgHqOqSL2bIMwM9SuOOK78~aJ0TQ2YGMS1Ta8v1SkJKv8BhDcaHc9YKXq~tDiJLSYnKcJ7sWTwZZLZLPW9tq3OVm4hLj27lLPkseC3LsSRHnx~6N2aFxzFeFTzFnvfWNwudGK6YRNy34hBaaI-Bn5~NoMXl2k20Cw8cBomntcg3e~C0ZrPhtQjajjYQ5Z-ws3G7SvPpWnGskCTg~xWX9D4MNdi16m~FV9WoNCeqScm8aKkU~1Hw__&Key-Pair-Id=APKAI7S6A5RYOXBWRPDA"

tar -xvzf longranger-2.2.2.tar.gzls

export PATH=/home/eoziolor/program/longranger-2.2.2:$PATH
```

## Checking MD5Sum of downloaded files

```{bash}
#!/bin/bash -l
#SBATCH -J md5check
#SBATCH -o md5sum-%j.o
#SBATCH -e md5sum-%j.o
#SBATCH -N 1
#SBATCH -n 8
#SBATCH --time=0-01:00
#SBATCH --mem=60000

md5sum /home/eoziolor/phgenome/data/raw/*.fastq.gz > /home/eoziolor/phgenome/data/raw/md5post.txt
```

* matches downloaded md5 sum

# Running ARCS pipeline

## Creating interleaved fastq

* Using proc10xG

```{bash}
git clone https://github.com/ucdavis-bioinformatics/proc10xG.git

###Creating trimmed interleaved files

#!/bin/bash -l
#SBATCH -J md5check
#SBATCH -o md5sum-%j.o
#SBATCH -e md5sum-%j.o
#SBATCH -N 1
#SBATCH -n 8
#SBATCH --time=2-00:00
#SBATCH --mem=60000

#programs and files
prog=/home/eoziolor/program/proc10xG/process_10xReads.py
file1=/home/eoziolor/phgenome/data/raw/PH-Sitka-93_S1_L008_R1_001.fastq.gz
file2=/home/eoziolor/phgenome/data/raw/PH-Sitka-93_S1_L008_R2_001.fastq.gz
output=/home/eoziolor/phgenome/data/trim/PH-Sitka-93_S1_L008_RA_001.fastq.gz

$prog \
-1 $file1 \
-2 $file2 \
-t 7 \
-o stdout | \
gzip > $output

```

##Longranger Basic

* gonna try this out instead because I need interleaved bam files.

```{bash}
#!/bin/bash -l
#SBATCH -J longbasic
#SBATCH -o longbasic-%j.o
#SBATCH -e longbasic-%j.o
#SBATCH -N 1
#SBATCH -n 8
#SBATCH --time=1-00:00
#SBATCH --mem=60000

#programs and files
long=/home/eoziolor/program/longranger-2.2.2/longranger
path=/home/eoziolor/phgenome/data/raw/
id=PH-Sitka-93_S1_L008

cd /home/eoziolor/phgenome/data/raw/
#code
$long basic \
--id=$id \
--fastqs=$path \
--bam
```

* Also creating a new fastq file...different parts of ARCS say different things about what you need

```{bash}
#!/bin/bash -l
#SBATCH -J fastqbasic
#SBATCH -o fastqbasic-%j.o
#SBATCH -e fastqbasic-%j.o
#SBATCH -N 1
#SBATCH -n 16
#SBATCH --time=2-00:00
#SBATCH --mem=60000

#programs and files
long=/home/eoziolor/program/longranger-2.2.2/longranger
path=/home/eoziolor/phgenome/data/raw/
id=NewFastq

cd /home/eoziolor/phgenome/data/raw/
#code
$long basic \
--id=$id \
--fastqs=$path

```

##REMEMBER TO ADD STEPS TO GET TO ARKS OUTPUT


###Arks run 1 comparison

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks1",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=264))/1000000
fai_10.1[264,2]/1000000

```

###ARKS run 2

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks2",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=252))/1000000
fai_10.1[252,2]/1000000

```

##Arks run 3

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks3",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=236))/1000000
fai_10.1[236,2]/1000000

```

##Arks run 4

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks4",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=228))/1000000
fai_10.1[228,2]/1000000
```

##Arks run 5

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks5",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=224))/1000000
fai_10.1[224,2]/1000000
```


##Arks run 6

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks6",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=225))/1000000
fai_10.1[225,2]/1000000
```

##Arks run 7

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks7",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=171))/1000000
fai_10.1[171,2]/1000000
```

##Arks run 8

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks8",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=123))/1000000
fai_10.1[123,2]/1000000
```

##Arks run 9

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks9",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=104))/1000000
fai_10.1[104,2]/1000000
```

##Arks run 10

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks10",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=76))/1000000
fai_10.1[76,2]/1000000
```

##Arks run 11

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks11",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=76))/1000000
fai_10.1[76,2]/1000000
```

##Arks run 12

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks12",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=70))/1000000
fai_10.1[70,2]/1000000
```

##Arks run 13

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/arks/arks13",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=65))/1000000
fai_10.1[65,2]/1000000
```

## Supernova assembly

```{r}
faisup<-read.table("~/phgenome_post/data/supernova/phgenome.hap.fa.fai",header=FALSE,stringsAsFactors = FALSE)
faisup10<-faisup[faisup[,2]>10000,]
ord<-order(faisup10[,2],decreasing=TRUE)
faisup10.1<-faisup10[ord,1:2]

faisup10.1<-faisup10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(faisup10.1)<-c("supscaf","size_bp","size_kb")

ggplot(faisup10.1,
       aes(x=size_kb))+
  scale_x_log10()+
  geom_histogram(bins=200)+
  theme_classic()

#Calculating N50
(sum(faisup10.1[,2])/1000000)/2
sum(head(faisup10.1[,2],n=250))/1000000
faisup10.1[250,2]/1000000
```

#### Now let's compare the two assemblies

```{r}
arks10k<-cbind("arks",melt(fai_10.1[,4]))
colnames(arks10k)<-c("scaf","size_kb")
sup10k<-cbind("sup",melt(faisup10.1[,3]))
colnames(sup10k)<-c("scaf","size_kb")

comb10k<-rbind(arks10k,sup10k)

ggplot(comb10k,
       aes(x=size_kb))+
  geom_histogram(data=subset(comb10k,scaf=="arks"),fill="red",bins=200,alpha=.5)+
  geom_histogram(data=subset(comb10k,scaf=="sup"),fill="blue",bins=200,alpha=.5)+
  scale_x_log10()+
#  scale_y_log10()+
  theme_classic()

plot(fai_10.1[1:1000,4],pch=20,cex=1,col="red")
points(faisup10.1[1:1000,3],pch=20,cex=1,col="blue")
```

####Looking at progress over time with the ARKS pipeline

```{r}
arks_files<-list.files("~/phgenome_post/data/arks", "arks*",full.names=TRUE)
arks<-list()

for(i in 1:length(arks_files)){
  arks[[i]]<-read.csv(arks_files[i],header=FALSE,stringsAsFactors = FALSE)
}
nam<-gsub(".*\\/","",arks_files)
names(arks)<-nam

ord<-c()
arks_ord<-list()

for(i in 1:length(arks_files)){
  arks[[i]][,4]<-arks[[i]][,2]/1000000
  ord<-order(arks[[i]][,4],decreasing=TRUE)
  arks_ord[[i]]<-arks[[i]][ord,]
}

plot(arks_ord[[length(arks_files)]][1:200,4],pch=20,cex=1)
for(i in 1:(length(arks_files)-1)){
  points(arks_ord[[i]][,4],pch=20,cex=1)
}

```

#### Plotting multiplicities from run2

```{r}
multi<-read.csv("~/phgenome_post/data/arks/multiplicities_run2.csv",header=FALSE,stringsAsFactors = FALSE)
small<-multi[multi[,2]<50,]
hist(small[,2],breaks=2000)
```

* No real reason to increase multiplicities range to meet more reads. None above 10000, the ones below 50 are pretty much lone pairs

## AltArks pipeline
##Arks run 7b

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/altarks/arks7b",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=1000)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=203))/1000000
fai_10.1[203,2]/1000000
```

##Arks run 8b

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/altarks/arks8b",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=200)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=196))/1000000
fai_10.1[196,2]/1000000
```

##Arks run 9b

```{r}
library(ggplot2)
library(dplyr)
library(magrittr)
library(reshape)

fai<-read.csv("~/phgenome_post/data/altarks/arks9b",header=FALSE,stringsAsFactors = FALSE) #reading in fai file produced from arks

fai_10<-fai[fai[,2]>10000,] #only selecting scaffolds above 10kb

ord<-order(fai_10[,2],decreasing=TRUE)
fai_10.1<-fai_10[ord,]

fai_10.1<-fai_10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(fai_10.1)<-c("arkscaf","size_bp","oldscaf","size_kb")

ggplot(fai_10.1,
       aes(x=size_kb))+
#  scale_y_log10()+
  scale_x_log10()+
  geom_histogram(bins=200)+
  theme_classic()

#calculating N50
(sum(fai_10.1[,2])/1000000)/2
sum(head(fai_10.1[,2],n=197))/1000000
fai_10.1[197,2]/1000000
```

## Supernova assembly

```{r}
faisup<-read.table("~/phgenome_post/data/supernova/phgenome.hap.fa.fai",header=FALSE,stringsAsFactors = FALSE)
faisup10<-faisup[faisup[,2]>10000,]
ord<-order(faisup10[,2],decreasing=TRUE)
faisup10.1<-faisup10[ord,1:2]

faisup10.1<-faisup10.1 %>% 
  mutate(size_kb=V2/1000)
colnames(faisup10.1)<-c("supscaf","size_bp","size_kb")

ggplot(faisup10.1,
       aes(x=size_kb))+
  scale_x_log10()+
  geom_histogram(bins=200)+
  theme_classic()

#Calculating N50
(sum(faisup10.1[,2])/1000000)/2
sum(head(faisup10.1[,2],n=250))/1000000
faisup10.1[250,2]/1000000
```

#### Now let's compare the two assemblies

```{r}
arks10k<-cbind("arks",melt(fai_10.1[,4]))
colnames(arks10k)<-c("scaf","size_kb")
sup10k<-cbind("sup",melt(faisup10.1[,3]))
colnames(sup10k)<-c("scaf","size_kb")

comb10k<-rbind(arks10k,sup10k)

ggplot(comb10k,
       aes(x=size_kb))+
  geom_histogram(data=subset(comb10k,scaf=="arks"),fill="red",bins=200,alpha=.5)+
  geom_histogram(data=subset(comb10k,scaf=="sup"),fill="blue",bins=200,alpha=.5)+
  scale_x_log10()+
#  scale_y_log10()+
  theme_classic()

plot(fai_10.1[1:1000,4],pch=20,cex=1,col="red")
points(faisup10.1[1:1000,3],pch=20,cex=1,col="blue")
```

####Looking at progress over time with the ARKS pipeline

```{r}
arks_files<-list.files("~/phgenome_post/data/altarks", "arks*",full.names=TRUE)
arks<-list()

for(i in 1:length(arks_files)){
  arks[[i]]<-read.csv(arks_files[i],header=FALSE,stringsAsFactors = FALSE)
}
nam<-gsub(".*\\/","",arks_files)
names(arks)<-nam

ord<-c()
arks_ord<-list()

for(i in 1:length(arks_files)){
  arks[[i]][,4]<-arks[[i]][,2]/1000000
  ord<-order(arks[[i]][,4],decreasing=TRUE)
  arks_ord[[i]]<-arks[[i]][ord,]
}

plot(arks_ord[[length(arks_files)]][1:200,4],pch=20,cex=1)
for(i in 1:(length(arks_files)-1)){
  points(arks_ord[[i]][,4],pch=20,cex=1)
}
```